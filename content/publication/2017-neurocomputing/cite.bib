@article{2017_neurocomputing,
 abstract = {Research on feature relevance and feature selection problems goes back several decades, but the importance of these areas continues to grow as more and more data becomes available, and machine learning methods are used to gain insight and interpret, rather than solely to solve classification or regression problems. Despite the fact that feature relevance is often discussed, it is frequently poorly defined, and the feature selection problems studied are subtly different. Furthermore, the problem of finding all features relevant for a classification problem has only recently started to gain traction, despite its importance for interpretability and integrating expert knowledge. In this paper, we attempt to unify commonly used concepts and to give an overview of the main questions and results. We formalize two interpretations of the all-relevant problem and propose a polynomial method to approximate one of them for the important hypothesis class of linear classifiers, which also enables a distinction between strongly and weakly relevant features.},
 author = {Göpfert, Christina and Pfannschmidt, Lukas and Göpfert, Jan Philip and Hammer, Barbara},
 doi = {10.1016/j.neucom.2017.11.074},
 journal = {Neurocomputing},
 keyword = {Feature Relevance, Feature Selection, Interpretability, All-Relevant, Linear Classification},
 pages = {69--79},
 publisher = {Elsevier},
 title = {Interpretation of Linear Classifiers by Means of Feature Relevance Bounds},
 volume = {298},
 year = {2018}
}

